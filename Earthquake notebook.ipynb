{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:13.911246900Z",
     "start_time": "2024-04-25T17:31:13.891774100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/User/PycharmProjects/Earthquake/Student Hiring Project 2017 - Training Data.txt'\n",
    "\n",
    "# Prepare a list to hold all the data\n",
    "data = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:14.426556700Z",
     "start_time": "2024-04-25T17:31:14.395548900Z"
    }
   },
   "id": "4c1e276c612cf42b",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip install pandas   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:15.109613100Z",
     "start_time": "2024-04-25T17:31:15.089735Z"
    }
   },
   "id": "6530c0ec9ef4e2c1",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Open the file and read line by line\n",
    "with open(file_path, 'r', encoding=\"utf-8-sig\") as file:\n",
    "    lines = file.readlines()\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "\n",
    "        # Split the line into parts and strip to remove any extra whitespace\n",
    "        parts = line.strip().split(',')\n",
    "\n",
    "        # Convert all but the last element to floats\n",
    "        measurements = np.array(parts[:-1], dtype=float)\n",
    "\n",
    "        # Extract the label from the last element\n",
    "        label = int(parts[-1])\n",
    "\n",
    "        # Calculate the required statistics\n",
    "        min_val = measurements.min()\n",
    "        max_val = measurements.max()\n",
    "        mean_val = measurements.mean()\n",
    "        std_val = measurements.std()\n",
    "\n",
    "        # Append the statistics and label to the data list\n",
    "        data.append([min_val, max_val, mean_val, std_val, label])\n",
    "        columns = ['Min', 'Max', 'Mean', 'Std', 'Label']\n",
    "        df = pd.DataFrame(data, columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:15.955474600Z",
     "start_time": "2024-04-25T17:31:15.773287800Z"
    }
   },
   "id": "dd7ace7c11302483",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_file_path = 'C:/Users/User/PycharmProjects/Earthquake/Student Hiring Project 2017 - Testing Data.txt'\n",
    "\n",
    "# Prepare a list to hold all the data\n",
    "test_data = []\n",
    "with open(test_file_path, 'r', encoding=\"utf-8-sig\") as file:\n",
    "    lines = file.readlines()\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        i += 1\n",
    "\n",
    "        # Split the line into parts and strip to remove any extra whitespace\n",
    "        parts = line.strip().split(',')\n",
    "\n",
    "        # Convert all but the last element to floats\n",
    "        measurements = np.array(parts[:-1], dtype=float)\n",
    "\n",
    "        # Extract the label from the last element\n",
    "        label = int(parts[-1])\n",
    "\n",
    "        # Calculate the required statistics\n",
    "        min_val = measurements.min()\n",
    "        max_val = measurements.max()\n",
    "        mean_val = measurements.mean()\n",
    "        std_val = measurements.std()\n",
    "\n",
    "        # Append the statistics and label to the data list\n",
    "        test_data.append([min_val, max_val, mean_val, std_val, label])\n",
    "        columns = ['Min', 'Max', 'Mean', 'Std', 'Label']\n",
    "        test_df = pd.DataFrame(test_data, columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:17.779623100Z",
     "start_time": "2024-04-25T17:31:17.712531Z"
    }
   },
   "id": "b27fa335a6f4a906",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Min     Max          Mean       Std  Label\n",
      "5   -0.24025  6.0425 -3.027344e-06  0.999024      0\n",
      "8   -0.20715  7.5787 -3.515625e-06  0.999026      0\n",
      "12  -0.20405  6.7908 -1.074219e-06  0.999024      0\n",
      "13  -0.26656  5.2691  1.718750e-06  0.999024      0\n",
      "21  -0.25160  6.4258  1.757813e-06  0.999023      0\n",
      "24  -0.19825  6.9052 -7.812500e-07  0.999024      1\n",
      "36  -0.25822  5.6711 -2.304688e-06  0.999023      0\n",
      "44  -0.28802  5.1532 -4.296875e-07  0.999021      0\n",
      "51  -0.21806  7.4394 -2.695312e-06  0.999025      0\n",
      "53  -0.33812  5.3003  3.593750e-06  0.999022      0\n",
      "67  -0.22808  6.7781 -1.914062e-06  0.999021      0\n",
      "69  -0.23762  5.4372 -3.867188e-06  0.999023      0\n",
      "93  -0.25316  6.8365 -4.687500e-06  0.999021      0\n",
      "97  -0.26738  5.3284 -4.609375e-06  0.999024      0\n",
      "111 -0.28814  6.3783 -7.812500e-08  0.999023      0\n",
      "126 -0.24918  5.7995  4.140625e-06  0.999023      0\n",
      "138 -0.29102  5.1573 -7.812500e-07  0.999023      0\n",
      "140 -0.26765  6.9440 -3.320313e-06  0.999024      0\n",
      "142 -0.30417  5.4708 -1.933594e-06  0.999021      0\n",
      "153 -0.29405  5.5313 -9.765625e-07  0.999017      0\n",
      "156 -0.24621  5.5147 -1.601562e-06  0.999023      0\n",
      "158 -0.29307  5.2701  2.148437e-06  0.999023      0\n",
      "160 -0.26247  5.6835  2.226562e-06  0.999022      0\n",
      "168 -0.36844  5.1304  2.695313e-06  0.999021      0\n",
      "169 -0.22537  6.4527  2.890625e-06  0.999024      0\n",
      "174 -0.28171  5.0113 -2.011719e-06  0.999025      0\n",
      "176 -0.25053  5.1557  4.042969e-06  0.999023      0\n",
      "181 -0.27352  6.2285  2.929688e-06  0.999022      0\n",
      "189 -0.22846  7.8149 -2.734375e-06  0.999027      0\n",
      "202 -0.29048  5.5689  7.812500e-07  0.999020      0\n",
      "204 -0.20381  7.0688  1.757813e-07  0.999023      0\n",
      "219 -0.24849  7.1873  1.582031e-06  0.999025      0\n",
      "221 -0.28034  5.1895 -3.906250e-08  0.999022      0\n",
      "226 -0.30419  5.1855 -5.156250e-06  0.999021      0\n",
      "233 -0.24398  5.8779 -7.031250e-07  0.999021      0\n",
      "235 -0.35407  5.2550 -3.398438e-06  0.999025      0\n",
      "240 -0.20955  5.9154  2.734375e-06  0.999018      0\n",
      "243 -0.34798  5.2138 -2.343750e-06  0.999024      0\n",
      "252 -0.28565  5.9061 -1.269531e-06  0.999024      0\n",
      "253 -0.28257  5.3907 -1.582031e-06  0.999023      0\n",
      "255 -0.26283  5.5916 -1.054688e-06  0.999023      0\n",
      "257 -0.34370  5.3659 -3.125000e-06  0.999021      0\n",
      "266 -0.24538  6.2704 -3.828125e-06  0.999025      0\n",
      "275 -0.29160  5.5137  1.757812e-06  0.999022      0\n",
      "279 -0.22285  6.0744  1.464844e-06  0.999022      0\n",
      "284 -0.20167  7.8634  2.402344e-06  0.999022      0\n",
      "288 -0.20329  6.5651  3.339844e-06  0.999024      0\n",
      "291 -0.31241  5.4586  2.832031e-06  0.999024      0\n",
      "293 -0.30564  5.3838 -2.890625e-06  0.999022      0\n",
      "299 -0.21338  6.8841 -2.382812e-06  0.999023      0\n",
      "300 -0.34562  5.2603 -9.375000e-07  0.999024      0\n",
      "305 -0.25173  7.0364  7.812500e-07  0.999024      0\n",
      "308 -0.26664  5.7057 -4.648437e-06  0.999024      0\n",
      "319 -0.26465  5.5489  1.660156e-06  0.999022      0\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df[df['Max'] > 5]\n",
    "print(filtered_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:28.801901600Z",
     "start_time": "2024-04-25T17:31:28.768262800Z"
    }
   },
   "id": "b6de9d901dea11fe",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Getting rid of index 24, this index doesn't fulfill the requirment that a major event occurence\n",
    " over the last 512 hours was labeled 1 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31b67be8f292282d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Dropping the index which doesn't meet the criterion\n",
    "df = df.drop(index=24)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:37.920466600Z",
     "start_time": "2024-04-25T17:31:37.892460400Z"
    }
   },
   "id": "142728c76c22ccb",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Label\n0    264\n1     57\nName: count, dtype: int64"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = df.iloc[:, -1].value_counts()\n",
    "label_counts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:40.024302400Z",
     "start_time": "2024-04-25T17:31:39.986217800Z"
    }
   },
   "id": "dbbf9f43810e6f83",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see some imbalance in the data. We will create 2 models one the original data and one with an \n",
    "oversampling of the minority class and we'll check which performs better.\n",
    "## Logistic regression with imbalanced data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe003b3e851ee1da"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90       264\n",
      "           1       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.82       321\n",
      "   macro avg       0.41      0.50      0.45       321\n",
      "weighted avg       0.68      0.82      0.74       321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Loading the DataFrame 'df' with the last column as the target\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Normalizing the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Setting up stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Creating a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Generating cross-validated predictions\n",
    "y_pred = cross_val_predict(model, X_scaled, y, cv=skf)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:44.863167500Z",
     "start_time": "2024-04-25T17:31:44.805961500Z"
    }
   },
   "id": "c829a62e29b4bf04",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upsampling the minority class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5ed730b9a0d536b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    264\n",
      "1    264\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separating majority and minority classes\n",
    "df_majority = df[df.Label == 0]\n",
    "df_minority = df[df.Label == 1]\n",
    "\n",
    "# Upsampling minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=264,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Combining majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Displaying new class counts\n",
    "print(df_upsampled.Label.value_counts())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:49.990013600Z",
     "start_time": "2024-04-25T17:31:49.966523900Z"
    }
   },
   "id": "86a0683a326f553c",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic regression with balanced data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b47f8cc4f44c7032"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.58      0.64       264\n",
      "           1       0.65      0.78      0.71       264\n",
      "\n",
      "    accuracy                           0.68       528\n",
      "   macro avg       0.69      0.68      0.67       528\n",
      "weighted avg       0.69      0.68      0.67       528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Loading the DataFrame 'df' with the last column as the target\n",
    "X = df_upsampled.iloc[:, :-1]\n",
    "y = df_upsampled.iloc[:, -1]\n",
    "\n",
    "# Normalizing the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Set up a stratified K-Fold for cross-validation to maintain the class balance in each fold\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Using cross_val_predict to get predictions from each fold of cross-validation\n",
    "y_pred = cross_val_predict(model, X_scaled, y, cv=cv)\n",
    "\n",
    "# Printing the classification report, which includes precision, recall, and F1-score\n",
    "print(classification_report(y, y_pred))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:53.493045400Z",
     "start_time": "2024-04-25T17:31:53.408992600Z"
    }
   },
   "id": "41ab71a57da0e2af",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see a drastic improvment in the f1 score and Recall in the model with the balanced dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "804eb6588e650ef9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking the metrics on the test set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f62801dcbcf5cbd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.55      0.67       104\n",
      "           1       0.36      0.74      0.48        35\n",
      "\n",
      "    accuracy                           0.60       139\n",
      "   macro avg       0.61      0.65      0.58       139\n",
      "weighted avg       0.74      0.60      0.62       139\n",
      "\n",
      "[[57 47]\n",
      " [ 9 26]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assume X_train, y_train are your entire training feature set and labels\n",
    "# X_test, y_test are your test feature set and labels\n",
    "X_train = df_upsampled.iloc[:, :-1]\n",
    "y_train = df_upsampled.iloc[:, -1]\n",
    "\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "# Fit the scaler on the training set\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set with the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model on the entire training set\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Generating a classification report\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(conf_matrix)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:32:25.955175200Z",
     "start_time": "2024-04-25T17:32:25.919377500Z"
    }
   },
   "id": "9964332d2e1e311c",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Random Forest...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81       104\n",
      "           1       0.33      0.20      0.25        35\n",
      "\n",
      "    accuracy                           0.70       139\n",
      "   macro avg       0.55      0.53      0.53       139\n",
      "weighted avg       0.65      0.70      0.67       139\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       104\n",
      "           1       0.43      0.34      0.38        35\n",
      "\n",
      "    accuracy                           0.72       139\n",
      "   macro avg       0.61      0.59      0.60       139\n",
      "weighted avg       0.70      0.72      0.71       139\n",
      "\n",
      "Evaluating SVM...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.58      0.69       104\n",
      "           1       0.36      0.71      0.48        35\n",
      "\n",
      "    accuracy                           0.61       139\n",
      "   macro avg       0.61      0.65      0.59       139\n",
      "weighted avg       0.73      0.61      0.64       139\n",
      "\n",
      "Evaluating Neural Network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.66      0.74       104\n",
      "           1       0.38      0.60      0.46        35\n",
      "\n",
      "    accuracy                           0.65       139\n",
      "   macro avg       0.60      0.63      0.60       139\n",
      "weighted avg       0.72      0.65      0.67       139\n",
      "\n",
      "Evaluating Naive Bayes...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.40      0.56       104\n",
      "           1       0.33      0.89      0.48        35\n",
      "\n",
      "    accuracy                           0.53       139\n",
      "   macro avg       0.62      0.64      0.52       139\n",
      "weighted avg       0.77      0.53      0.54       139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X_train = df_upsampled.iloc[:, :-1]\n",
    "y_train = df_upsampled.iloc[:, -1]\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "# Define a dictionary of models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(),\n",
    "    'Neural Network': MLPClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Fit the scaler on the training set\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Iterating over models and evaluate each one\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:53:21.768052600Z",
     "start_time": "2024-04-25T17:53:20.721891500Z"
    }
   },
   "id": "dec3c792b8208716",
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
