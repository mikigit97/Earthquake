{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:27:10.917981600Z",
     "start_time": "2024-04-25T23:27:09.558136Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/User/PycharmProjects/Earthquake/Student Hiring Project 2017 - Training Data.txt'\n",
    "\n",
    "# Prepare a list to hold all the data\n",
    "data = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:27:13.793126300Z",
     "start_time": "2024-04-25T23:27:13.576056100Z"
    }
   },
   "id": "4c1e276c612cf42b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Open the file and read line by line\n",
    "with open(file_path, 'r', encoding=\"utf-8-sig\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "\n",
    "        # Splitting the line into parts and strip to remove any extra whitespace\n",
    "        parts = line.strip().split(',')\n",
    "\n",
    "        # Converting all but the last element to floats\n",
    "        measurements = np.array(parts[:-1], dtype=float)\n",
    "\n",
    "        # Extracting the label from the last element\n",
    "        label = int(parts[-1])\n",
    "\n",
    "        # Calculating the required statistics\n",
    "        min_val = measurements.min()\n",
    "        max_val = measurements.max()\n",
    "        mean_val = measurements.mean()\n",
    "        std_val = measurements.std()\n",
    "\n",
    "        # Append the statistics and label to the data list\n",
    "        data.append([min_val, max_val, mean_val, std_val, label])\n",
    "        columns = ['Min', 'Max', 'Mean', 'Std', 'Label']\n",
    "        df_featured = pd.DataFrame(data, columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:47:25.761987400Z",
     "start_time": "2024-04-25T23:47:25.481219500Z"
    }
   },
   "id": "dd7ace7c11302483",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b82fc6afabb248"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Assuming 'data.csv' is your data file where each row has 512 measurements + 1 label\n",
    "df_raw = pd.read_csv('C:/Users/User/PycharmProjects/Earthquake/Student Hiring Project 2017 - Training Data.txt', header=None)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df_raw.iloc[:, :-1].values  # all rows, all columns except the last one\n",
    "y = df_raw.iloc[:, -1].values   # all rows, only the last column\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the data for LSTM: [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:56:24.229943400Z",
     "start_time": "2024-04-25T23:56:24.139529200Z"
    }
   },
   "id": "5c2d6b5947233c15",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "lstm_model = Sequential()\n",
    "tf.keras.backend.experimental.enable_tf_random_generator()\n",
    "lstm_model.add(LSTM(50, input_shape=(512, 1)))  # 50 LSTM units, 512 time steps, 1 feature per step\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:56:33.477628400Z",
     "start_time": "2024-04-25T23:56:32.619879500Z"
    }
   },
   "id": "30f2b893a00ad13f",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 12s 1s/step - loss: 0.6809 - accuracy: 0.6732 - val_loss: 0.6512 - val_accuracy: 0.8846\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 3s 417ms/step - loss: 0.6498 - accuracy: 0.7707 - val_loss: 0.6008 - val_accuracy: 0.9038\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 3s 421ms/step - loss: 0.5972 - accuracy: 0.7854 - val_loss: 0.4649 - val_accuracy: 0.8654\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 3s 443ms/step - loss: 0.4894 - accuracy: 0.8195 - val_loss: 0.3652 - val_accuracy: 0.8654\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 3s 463ms/step - loss: 0.4826 - accuracy: 0.8146 - val_loss: 0.3702 - val_accuracy: 0.8654\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 3s 433ms/step - loss: 0.4731 - accuracy: 0.8146 - val_loss: 0.3779 - val_accuracy: 0.8654\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 3s 377ms/step - loss: 0.4699 - accuracy: 0.8146 - val_loss: 0.3724 - val_accuracy: 0.8654\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 3s 364ms/step - loss: 0.4683 - accuracy: 0.8146 - val_loss: 0.3646 - val_accuracy: 0.8654\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 3s 371ms/step - loss: 0.4682 - accuracy: 0.8146 - val_loss: 0.3657 - val_accuracy: 0.8654\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 3s 455ms/step - loss: 0.4657 - accuracy: 0.8098 - val_loss: 0.3667 - val_accuracy: 0.8654\n",
      "Test accuracy: 0.8153846263885498, Test loss: 0.44395098090171814\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test accuracy: {test_accuracy}, Test loss: {test_loss}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:57:24.325692200Z",
     "start_time": "2024-04-25T23:56:46.227810500Z"
    }
   },
   "id": "2421cae637edb4ce",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_file_path = 'C:/Users/User/PycharmProjects/Earthquake/Student Hiring Project 2017 - Testing Data.txt'\n",
    "\n",
    "# Prepare a list to hold all the data\n",
    "test_data = []\n",
    "with open(test_file_path, 'r', encoding=\"utf-8-sig\") as file:\n",
    "    lines = file.readlines()\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        i += 1\n",
    "\n",
    "        # Split the line into parts and strip to remove any extra whitespace\n",
    "        parts = line.strip().split(',')\n",
    "\n",
    "        # Convert all but the last element to floats\n",
    "        measurements = np.array(parts[:-1], dtype=float)\n",
    "\n",
    "        # Extract the label from the last element\n",
    "        label = int(parts[-1])\n",
    "\n",
    "        # Calculate the required statistics\n",
    "        min_val = measurements.min()\n",
    "        max_val = measurements.max()\n",
    "        mean_val = measurements.mean()\n",
    "        std_val = measurements.std()\n",
    "\n",
    "        # Append the statistics and label to the data list\n",
    "        test_data.append([min_val, max_val, mean_val, std_val, label])\n",
    "        columns = ['Min', 'Max', 'Mean', 'Std', 'Label']\n",
    "        test_df = pd.DataFrame(test_data, columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:49:05.908112700Z",
     "start_time": "2024-04-25T23:49:05.764821Z"
    }
   },
   "id": "b27fa335a6f4a906",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Min     Max          Mean       Std  Label\n",
      "5   -0.24025  6.0425 -3.027344e-06  0.999024      0\n",
      "8   -0.20715  7.5787 -3.515625e-06  0.999026      0\n",
      "12  -0.20405  6.7908 -1.074219e-06  0.999024      0\n",
      "13  -0.26656  5.2691  1.718750e-06  0.999024      0\n",
      "21  -0.25160  6.4258  1.757813e-06  0.999023      0\n",
      "24  -0.19825  6.9052 -7.812500e-07  0.999024      1\n",
      "36  -0.25822  5.6711 -2.304688e-06  0.999023      0\n",
      "44  -0.28802  5.1532 -4.296875e-07  0.999021      0\n",
      "51  -0.21806  7.4394 -2.695312e-06  0.999025      0\n",
      "53  -0.33812  5.3003  3.593750e-06  0.999022      0\n",
      "67  -0.22808  6.7781 -1.914062e-06  0.999021      0\n",
      "69  -0.23762  5.4372 -3.867188e-06  0.999023      0\n",
      "93  -0.25316  6.8365 -4.687500e-06  0.999021      0\n",
      "97  -0.26738  5.3284 -4.609375e-06  0.999024      0\n",
      "111 -0.28814  6.3783 -7.812500e-08  0.999023      0\n",
      "126 -0.24918  5.7995  4.140625e-06  0.999023      0\n",
      "138 -0.29102  5.1573 -7.812500e-07  0.999023      0\n",
      "140 -0.26765  6.9440 -3.320313e-06  0.999024      0\n",
      "142 -0.30417  5.4708 -1.933594e-06  0.999021      0\n",
      "153 -0.29405  5.5313 -9.765625e-07  0.999017      0\n",
      "156 -0.24621  5.5147 -1.601562e-06  0.999023      0\n",
      "158 -0.29307  5.2701  2.148437e-06  0.999023      0\n",
      "160 -0.26247  5.6835  2.226562e-06  0.999022      0\n",
      "168 -0.36844  5.1304  2.695313e-06  0.999021      0\n",
      "169 -0.22537  6.4527  2.890625e-06  0.999024      0\n",
      "174 -0.28171  5.0113 -2.011719e-06  0.999025      0\n",
      "176 -0.25053  5.1557  4.042969e-06  0.999023      0\n",
      "181 -0.27352  6.2285  2.929688e-06  0.999022      0\n",
      "189 -0.22846  7.8149 -2.734375e-06  0.999027      0\n",
      "202 -0.29048  5.5689  7.812500e-07  0.999020      0\n",
      "204 -0.20381  7.0688  1.757813e-07  0.999023      0\n",
      "219 -0.24849  7.1873  1.582031e-06  0.999025      0\n",
      "221 -0.28034  5.1895 -3.906250e-08  0.999022      0\n",
      "226 -0.30419  5.1855 -5.156250e-06  0.999021      0\n",
      "233 -0.24398  5.8779 -7.031250e-07  0.999021      0\n",
      "235 -0.35407  5.2550 -3.398438e-06  0.999025      0\n",
      "240 -0.20955  5.9154  2.734375e-06  0.999018      0\n",
      "243 -0.34798  5.2138 -2.343750e-06  0.999024      0\n",
      "252 -0.28565  5.9061 -1.269531e-06  0.999024      0\n",
      "253 -0.28257  5.3907 -1.582031e-06  0.999023      0\n",
      "255 -0.26283  5.5916 -1.054688e-06  0.999023      0\n",
      "257 -0.34370  5.3659 -3.125000e-06  0.999021      0\n",
      "266 -0.24538  6.2704 -3.828125e-06  0.999025      0\n",
      "275 -0.29160  5.5137  1.757812e-06  0.999022      0\n",
      "279 -0.22285  6.0744  1.464844e-06  0.999022      0\n",
      "284 -0.20167  7.8634  2.402344e-06  0.999022      0\n",
      "288 -0.20329  6.5651  3.339844e-06  0.999024      0\n",
      "291 -0.31241  5.4586  2.832031e-06  0.999024      0\n",
      "293 -0.30564  5.3838 -2.890625e-06  0.999022      0\n",
      "299 -0.21338  6.8841 -2.382812e-06  0.999023      0\n",
      "300 -0.34562  5.2603 -9.375000e-07  0.999024      0\n",
      "305 -0.25173  7.0364  7.812500e-07  0.999024      0\n",
      "308 -0.26664  5.7057 -4.648437e-06  0.999024      0\n",
      "319 -0.26465  5.5489  1.660156e-06  0.999022      0\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df_featured[df_featured['Max'] > 5]\n",
    "print(filtered_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:28.801901600Z",
     "start_time": "2024-04-25T17:31:28.768262800Z"
    }
   },
   "id": "b6de9d901dea11fe",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Getting rid of index 24, this index doesn't fulfill the requirment that a major event occurence\n",
    " over the last 512 hours was labeled 1 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31b67be8f292282d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Dropping the index which doesn't meet the criterion\n",
    "df_featured = df_featured.drop(index=24)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:47:38.159482700Z",
     "start_time": "2024-04-25T23:47:38.126143200Z"
    }
   },
   "id": "142728c76c22ccb",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Label\n0    264\n1     57\nName: count, dtype: int64"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = df_featured.iloc[:, -1].value_counts()\n",
    "label_counts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:40.024302400Z",
     "start_time": "2024-04-25T17:31:39.986217800Z"
    }
   },
   "id": "dbbf9f43810e6f83",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see some imbalance in the data. We will create 2 models one the original data and one with an \n",
    "oversampling of the minority class and we'll check which performs better.\n",
    "## Logistic regression with imbalanced data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe003b3e851ee1da"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90       264\n",
      "           1       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.82       321\n",
      "   macro avg       0.41      0.50      0.45       321\n",
      "weighted avg       0.68      0.82      0.74       321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Loading the DataFrame 'df' with the last column as the target\n",
    "X = df_featured.iloc[:, :-1]\n",
    "y = df_featured.iloc[:, -1]\n",
    "\n",
    "# Normalizing the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Setting up stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Creating a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Generating cross-validated predictions\n",
    "y_pred = cross_val_predict(model, X_scaled, y, cv=skf)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:44.863167500Z",
     "start_time": "2024-04-25T17:31:44.805961500Z"
    }
   },
   "id": "c829a62e29b4bf04",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upsampling the minority class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5ed730b9a0d536b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    264\n",
      "1    264\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separating majority and minority classes\n",
    "df_majority = df_featured[df_featured.Label == 0]\n",
    "df_minority = df_featured[df_featured.Label == 1]\n",
    "\n",
    "# Upsampling minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=264,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Combining majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Displaying new class counts\n",
    "print(df_upsampled.Label.value_counts())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:48:11.830947100Z",
     "start_time": "2024-04-25T23:48:11.772112100Z"
    }
   },
   "id": "86a0683a326f553c",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic regression with balanced data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b47f8cc4f44c7032"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.58      0.64       264\n",
      "           1       0.65      0.78      0.71       264\n",
      "\n",
      "    accuracy                           0.68       528\n",
      "   macro avg       0.69      0.68      0.67       528\n",
      "weighted avg       0.69      0.68      0.67       528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Loading the DataFrame 'df' with the last column as the target\n",
    "X = df_upsampled.iloc[:, :-1]\n",
    "y = df_upsampled.iloc[:, -1]\n",
    "\n",
    "# Normalizing the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Set up a stratified K-Fold for cross-validation to maintain the class balance in each fold\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Using cross_val_predict to get predictions from each fold of cross-validation\n",
    "y_pred = cross_val_predict(model, X_scaled, y, cv=cv)\n",
    "\n",
    "# Printing the classification report, which includes precision, recall, and F1-score\n",
    "print(classification_report(y, y_pred))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:53.493045400Z",
     "start_time": "2024-04-25T17:31:53.408992600Z"
    }
   },
   "id": "41ab71a57da0e2af",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see a drastic improvment in the f1 score and Recall in the model with the balanced dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "804eb6588e650ef9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking the metrics on the test set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f62801dcbcf5cbd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.55      0.67       104\n",
      "           1       0.36      0.74      0.48        35\n",
      "\n",
      "    accuracy                           0.60       139\n",
      "   macro avg       0.61      0.65      0.58       139\n",
      "weighted avg       0.74      0.60      0.62       139\n",
      "\n",
      "[[57 47]\n",
      " [ 9 26]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assume X_train, y_train are your entire training feature set and labels\n",
    "# X_test, y_test are your test feature set and labels\n",
    "X_train = df_upsampled.iloc[:, :-1]\n",
    "y_train = df_upsampled.iloc[:, -1]\n",
    "\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "# Fit the scaler on the training set\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set with the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model on the entire training set\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Generating a classification report\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(conf_matrix)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:32:25.955175200Z",
     "start_time": "2024-04-25T17:32:25.919377500Z"
    }
   },
   "id": "9964332d2e1e311c",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Random Forest...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81       104\n",
      "           1       0.33      0.20      0.25        35\n",
      "\n",
      "    accuracy                           0.70       139\n",
      "   macro avg       0.55      0.53      0.53       139\n",
      "weighted avg       0.65      0.70      0.67       139\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       104\n",
      "           1       0.43      0.34      0.38        35\n",
      "\n",
      "    accuracy                           0.72       139\n",
      "   macro avg       0.61      0.59      0.60       139\n",
      "weighted avg       0.70      0.72      0.71       139\n",
      "\n",
      "Evaluating SVM...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.58      0.69       104\n",
      "           1       0.36      0.71      0.48        35\n",
      "\n",
      "    accuracy                           0.61       139\n",
      "   macro avg       0.61      0.65      0.59       139\n",
      "weighted avg       0.73      0.61      0.64       139\n",
      "\n",
      "Evaluating Neural Network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.66      0.74       104\n",
      "           1       0.38      0.60      0.46        35\n",
      "\n",
      "    accuracy                           0.65       139\n",
      "   macro avg       0.60      0.63      0.60       139\n",
      "weighted avg       0.72      0.65      0.67       139\n",
      "\n",
      "Evaluating Naive Bayes...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.40      0.56       104\n",
      "           1       0.33      0.89      0.48        35\n",
      "\n",
      "    accuracy                           0.53       139\n",
      "   macro avg       0.62      0.64      0.52       139\n",
      "weighted avg       0.77      0.53      0.54       139\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.51      0.63       104\n",
      "           1       0.33      0.71      0.45        35\n",
      "\n",
      "    accuracy                           0.56       139\n",
      "   macro avg       0.59      0.61      0.54       139\n",
      "weighted avg       0.71      0.56      0.59       139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\mycondaenv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X_train = df_upsampled.iloc[:, :-1]\n",
    "y_train = df_upsampled.iloc[:, -1]\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "# Define a dictionary of models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(),\n",
    "    'Neural Network': MLPClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Logistic Regression': LogisticRegression()\n",
    "}\n",
    "\n",
    "# Fit the scaler on the training set\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Iterating over models and evaluate each one\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:50:35.616919200Z",
     "start_time": "2024-04-25T23:50:32.700916400Z"
    }
   },
   "id": "dec3c792b8208716",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.47      0.57       264\n",
      "           1       0.61      0.84      0.71       264\n",
      "\n",
      "    accuracy                           0.66       528\n",
      "   macro avg       0.68      0.66      0.64       528\n",
      "weighted avg       0.68      0.66      0.64       528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Loading the DataFrame 'df' with the last column as the target\n",
    "X = df_upsampled.iloc[:, :-1]\n",
    "y = df_upsampled.iloc[:, -1]\n",
    "\n",
    "# Normalizing the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Set up a stratified K-Fold for cross-validation to maintain the class balance in each fold\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a GaussianNB model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Using cross_val_predict to get predictions from each fold of cross-validation\n",
    "y_pred = cross_val_predict(nb_model, X_scaled, y, cv=cv)\n",
    "\n",
    "# Printing the classification report, which includes precision, recall, and F1-score\n",
    "print(classification_report(y, y_pred))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T00:08:10.052189100Z",
     "start_time": "2024-04-26T00:08:09.965532800Z"
    }
   },
   "id": "56a5735ee8e04984",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cad53e86215f38f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
